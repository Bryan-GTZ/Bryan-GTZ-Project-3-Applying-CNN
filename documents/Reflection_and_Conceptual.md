
## a) How did you create your dataset and determine split of official logo versus not official logo?
We built our dataset by finding 50 images on Etsy. We focused on keeping it simple for the initial model, splitting the images into two folders. This imbalance favoring the official logo could be a source of potential bias, but we feel as though it is not significant. For splitting the data, we used a 70% to 30% ratio, which gave us 35 images for training and 15 images for validation. This 70/30 split, secured by using a fixed random seed, gave the model a good number of examples to learn the logo's subtle features, which helps prevent underfitting. Crucially, the 15-image validation set is much more reliable for evaluation than the 10 images we would have gotten from a 20% split, ensuring our accuracy metric isn't overly unstable.

## b) Why did you choose the specific architecture for the final model?
I chose this custom CNN because it effectively balances complexity with our small dataset size. It uses multiple convolutional and max pooling layers to efficiently extract key features like edges and contours of the logo, making the model robust. Crucially, the architecture is simple and fast enough for local training, while including dropout layers to generalize well and avoid the severe overfitting that a much deeper network would cause with a dataset this size.
## c) How did you monitor and mitigate overfitting?
I monitored overfitting by tracking the Training vs. Validation Loss and Accuracy curves throughout the training process. A key indicator of divergence was observing the training accuracy continuing to rise while validation accuracy began to drop. To prevent this, I used data augmentation to artificially increase the size and diversity of the dataset, dropout layers to prevent adaptation, and early stopping to stop training when validation performance stopped improving. These steps collectively reduced model complexity and significantly improved its ability to generalize to new images.
## d) What future efforts do you recommend to improve model performance?
Future model performance can be improved primarily by expanding and diversifying the dataset, adding more examples of both official logos and other unrelated images to help the model learn subtle distinctions and generalize better. Applying more advanced data augmentation techniques like such as perspective shifts or controlled distortions would also help the network handle the wide variety of images. Finally, improving preprocessing consistency by standardizing image resolution, aspect ratio, and cropping around the main subject would reduce noise and ensure the model focuses on the logo rather than irrelevant background details.
