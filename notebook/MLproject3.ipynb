{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets, transforms, models\n",
    "\n",
    "# added the packages that are needed for THE ASSIGNMENT HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Configuration parameters\n",
    "# to facilitate the code i added these variables so we can change them easily\n",
    "DATA_DIR = \"/Users/bryangutierrez/Desktop/project3_ML\"   \n",
    "BATCH_SIZE = 8\n",
    "NUM_EPOCHS = 50\n",
    "LEARNING_RATE = 1e-4\n",
    "VAL_SPLIT = 0.3\n",
    "RANDOM_SEED = 43\n",
    " \n",
    "MODEL_PATH = \"Group_16_CNN_FullModel.ph\"\n",
    "\n",
    "# THis to make the code reproducible\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "random.seed(RANDOM_SEED)\n",
    "\n",
    "# this is to detect whether your computer has a GPU which is Cuda available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data transformations\n",
    "# this prepares the images so the cnn can process them \n",
    "# rezize, normalize, augment the data with random horizontal flip\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((500, 500)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.ToTensor(),\n",
    "\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "# Validation transformations\n",
    "# this prepares the images so the cnn can process them \n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((500, 500)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: ['not_official', 'official']\n",
      "Total images: 50\n",
      "Train size: 35, Val size: 15\n"
     ]
    }
   ],
   "source": [
    "# Load the full dataset\n",
    "full_dataset = datasets.ImageFolder(root=DATA_DIR, transform=train_transform)\n",
    "\n",
    "class_names = full_dataset.classes\n",
    "print(\"Classes:\", class_names)  \n",
    "\n",
    "# Train/val split\n",
    "# this splits the data into training and validation sets\n",
    "dataset_size = len(full_dataset)\n",
    "val_size = int(VAL_SPLIT * dataset_size)\n",
    "train_size = dataset_size - val_size\n",
    "# this makes the split reproducible by setting a manual seed\n",
    "train_dataset, val_dataset = random_split(\n",
    "    full_dataset,\n",
    "    [train_size, val_size],\n",
    "    generator=torch.Generator().manual_seed(RANDOM_SEED)\n",
    ")\n",
    "\n",
    "# Update transforms for validation dataset\n",
    "# this applies the validation transformations to the validation dataset\n",
    "val_dataset.dataset.transform = val_transform\n",
    "# Data loaders which handle batching and shuffling\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(f\"Total images: {dataset_size}\")\n",
    "print(f\"Train size: {train_size}, Val size: {val_size}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RazorbackCNN(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU(inplace=True)\n",
      "    (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (9): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (10): ReLU(inplace=True)\n",
      "    (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (12): AdaptiveAvgPool2d(output_size=(4, 4))\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (1): Linear(in_features=2048, out_features=256, bias=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Dropout(p=0.5, inplace=False)\n",
      "    (4): Linear(in_features=256, out_features=2, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Define the CNN model\n",
    "# This is a custom CNN architecture named RazorbackCNN\n",
    "class RazorbackCNN(nn.Module):\n",
    "\n",
    "\n",
    "   # this is the constructor of the class\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(RazorbackCNN, self).__init__()\n",
    "\n",
    "# this defines the layers of the CNN\n",
    "\n",
    "        self.features = nn.Sequential(\n",
    "            # Block 1\n",
    "\n",
    "\n",
    "            # this block processes the input image\n",
    "            nn.Conv2d(3, 16, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),  # 500 -> 250\n",
    "\n",
    "            # Block 2\n",
    "            # this block extracts more complex features\n",
    "            nn.Conv2d(16, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),  # 250 -> 125\n",
    "\n",
    "            # Block 3\n",
    "            # this block further refines the features\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),  # 125 -> 62/63\n",
    "\n",
    "            # Block 4\n",
    "            # this block captures high-level features\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),  # ~62 -> ~31\n",
    "\n",
    "           \n",
    "            nn.AdaptiveAvgPool2d((4, 4))\n",
    "        )\n",
    "# this defines the fully connected layers for classification\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128 * 4 * 4, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, num_classes)  # 2 classes\n",
    "        )\n",
    "# this defines the forward pass of the model\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "model = RazorbackCNN(num_classes=2).to(device) # move the model to the appropriate device\n",
    "print(model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function and optimizer\n",
    "# this defines the loss function and optimizer for training\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and evaluation functions\n",
    "def train_one_epoch(model, dataloader, criterion, optimizer, device): # this function trains the model for one epoch\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    total = 0\n",
    "\n",
    "\n",
    "# this loops over the data in the dataloader\n",
    "    for images, labels in dataloader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad() # reset gradients\n",
    "\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "\n",
    "# computes the predictions and updates the loss and accuracy\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "# updates the running loss and correct predictions\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        running_corrects += torch.sum(preds == labels).item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "\n",
    "# computes the epoch loss and accuracy\n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = running_corrects / total\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "# this function evaluates the model on the validation set\n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    total = 0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#  loops over the data in the dataloader without computing gradients\n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(images) # get model outputs\n",
    "            loss = criterion(outputs, labels) # computes the loss\n",
    "\n",
    "            _, preds = torch.max(outputs, 1) # computes the predictions\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            running_corrects += torch.sum(preds == labels).item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "\n",
    "# THUS computes the epoch loss and accuracy\n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = running_corrects / total\n",
    "    return epoch_loss, epoch_acc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50] Train Loss: 0.6859 | Train Acc: 0.6286 || Val Loss: 0.6722 | Val Acc: 0.6667\n",
      "Epoch [2/50] Train Loss: 0.6617 | Train Acc: 0.7143 || Val Loss: 0.6425 | Val Acc: 0.6667\n",
      "Epoch [3/50] Train Loss: 0.6277 | Train Acc: 0.7143 || Val Loss: 0.6196 | Val Acc: 0.6667\n",
      "Epoch [4/50] Train Loss: 0.6028 | Train Acc: 0.7143 || Val Loss: 0.6132 | Val Acc: 0.6667\n",
      "Epoch [5/50] Train Loss: 0.5715 | Train Acc: 0.7143 || Val Loss: 0.6100 | Val Acc: 0.6667\n",
      "Epoch [6/50] Train Loss: 0.5731 | Train Acc: 0.7143 || Val Loss: 0.6151 | Val Acc: 0.6667\n",
      "Epoch [7/50] Train Loss: 0.5789 | Train Acc: 0.7143 || Val Loss: 0.6193 | Val Acc: 0.6667\n",
      "Epoch [8/50] Train Loss: 0.5745 | Train Acc: 0.7143 || Val Loss: 0.6214 | Val Acc: 0.6667\n",
      "Epoch [9/50] Train Loss: 0.5484 | Train Acc: 0.7143 || Val Loss: 0.6211 | Val Acc: 0.6667\n",
      "Epoch [10/50] Train Loss: 0.5746 | Train Acc: 0.7143 || Val Loss: 0.6161 | Val Acc: 0.6667\n",
      "Epoch [11/50] Train Loss: 0.5607 | Train Acc: 0.7143 || Val Loss: 0.6128 | Val Acc: 0.6667\n",
      "Epoch [12/50] Train Loss: 0.5469 | Train Acc: 0.7143 || Val Loss: 0.6147 | Val Acc: 0.6667\n",
      "Epoch [13/50] Train Loss: 0.5697 | Train Acc: 0.7143 || Val Loss: 0.6147 | Val Acc: 0.6667\n",
      "Epoch [14/50] Train Loss: 0.5554 | Train Acc: 0.7143 || Val Loss: 0.6190 | Val Acc: 0.6667\n",
      "Epoch [15/50] Train Loss: 0.5446 | Train Acc: 0.7143 || Val Loss: 0.6217 | Val Acc: 0.6667\n",
      "Epoch [16/50] Train Loss: 0.5417 | Train Acc: 0.7143 || Val Loss: 0.6266 | Val Acc: 0.6667\n",
      "Epoch [17/50] Train Loss: 0.5256 | Train Acc: 0.7143 || Val Loss: 0.6142 | Val Acc: 0.6667\n",
      "Epoch [18/50] Train Loss: 0.5278 | Train Acc: 0.7143 || Val Loss: 0.6030 | Val Acc: 0.6667\n",
      "Epoch [19/50] Train Loss: 0.5270 | Train Acc: 0.7143 || Val Loss: 0.6015 | Val Acc: 0.6667\n",
      "Epoch [20/50] Train Loss: 0.5364 | Train Acc: 0.7143 || Val Loss: 0.6014 | Val Acc: 0.6667\n",
      "Epoch [21/50] Train Loss: 0.5108 | Train Acc: 0.7143 || Val Loss: 0.6080 | Val Acc: 0.6667\n",
      "Epoch [22/50] Train Loss: 0.4963 | Train Acc: 0.7143 || Val Loss: 0.6118 | Val Acc: 0.6667\n",
      "Epoch [23/50] Train Loss: 0.4870 | Train Acc: 0.7143 || Val Loss: 0.6109 | Val Acc: 0.6667\n",
      "Epoch [24/50] Train Loss: 0.4796 | Train Acc: 0.7143 || Val Loss: 0.5978 | Val Acc: 0.6667\n",
      "Epoch [25/50] Train Loss: 0.4965 | Train Acc: 0.7143 || Val Loss: 0.5845 | Val Acc: 0.6667\n",
      "Epoch [26/50] Train Loss: 0.4802 | Train Acc: 0.7429 || Val Loss: 0.6198 | Val Acc: 0.6667\n",
      "Epoch [27/50] Train Loss: 0.4863 | Train Acc: 0.7143 || Val Loss: 0.6029 | Val Acc: 0.6667\n",
      "Epoch [28/50] Train Loss: 0.4182 | Train Acc: 0.7429 || Val Loss: 0.5769 | Val Acc: 0.6667\n",
      "Epoch [29/50] Train Loss: 0.4288 | Train Acc: 0.8286 || Val Loss: 0.5663 | Val Acc: 0.6667\n",
      "Epoch [30/50] Train Loss: 0.4098 | Train Acc: 0.7714 || Val Loss: 0.5809 | Val Acc: 0.6667\n",
      "Epoch [31/50] Train Loss: 0.4087 | Train Acc: 0.7714 || Val Loss: 0.6197 | Val Acc: 0.6667\n",
      "Epoch [32/50] Train Loss: 0.3779 | Train Acc: 0.8286 || Val Loss: 0.5727 | Val Acc: 0.6667\n",
      "Epoch [33/50] Train Loss: 0.3719 | Train Acc: 0.8857 || Val Loss: 0.5781 | Val Acc: 0.6000\n",
      "Epoch [34/50] Train Loss: 0.3641 | Train Acc: 0.8857 || Val Loss: 0.5629 | Val Acc: 0.6000\n",
      "Epoch [35/50] Train Loss: 0.3160 | Train Acc: 0.8857 || Val Loss: 0.5964 | Val Acc: 0.6667\n",
      "Epoch [36/50] Train Loss: 0.3455 | Train Acc: 0.8857 || Val Loss: 0.6266 | Val Acc: 0.6667\n",
      "Epoch [37/50] Train Loss: 0.3425 | Train Acc: 0.8857 || Val Loss: 0.5789 | Val Acc: 0.6000\n",
      "Epoch [38/50] Train Loss: 0.3018 | Train Acc: 0.8571 || Val Loss: 0.5786 | Val Acc: 0.6000\n",
      "Epoch [39/50] Train Loss: 0.2677 | Train Acc: 0.8857 || Val Loss: 0.6019 | Val Acc: 0.6000\n",
      "Epoch [40/50] Train Loss: 0.2538 | Train Acc: 0.8857 || Val Loss: 0.5845 | Val Acc: 0.5333\n",
      "Epoch [41/50] Train Loss: 0.2305 | Train Acc: 0.9143 || Val Loss: 0.5772 | Val Acc: 0.6000\n",
      "Epoch [42/50] Train Loss: 0.2262 | Train Acc: 0.8857 || Val Loss: 0.5897 | Val Acc: 0.6000\n",
      "Epoch [43/50] Train Loss: 0.2336 | Train Acc: 0.9143 || Val Loss: 0.5859 | Val Acc: 0.6000\n",
      "Epoch [44/50] Train Loss: 0.2099 | Train Acc: 0.8857 || Val Loss: 0.6005 | Val Acc: 0.6667\n",
      "Epoch [45/50] Train Loss: 0.2298 | Train Acc: 0.9143 || Val Loss: 0.6228 | Val Acc: 0.6667\n",
      "Epoch [46/50] Train Loss: 0.2039 | Train Acc: 0.8571 || Val Loss: 0.5903 | Val Acc: 0.7333\n",
      "Epoch [47/50] Train Loss: 0.1930 | Train Acc: 0.9143 || Val Loss: 0.6076 | Val Acc: 0.6667\n",
      "Epoch [48/50] Train Loss: 0.1851 | Train Acc: 0.9143 || Val Loss: 0.6123 | Val Acc: 0.6667\n",
      "Epoch [49/50] Train Loss: 0.1580 | Train Acc: 0.9714 || Val Loss: 0.6063 | Val Acc: 0.6667\n",
      "Epoch [50/50] Train Loss: 0.1677 | Train Acc: 0.9429 || Val Loss: 0.6323 | Val Acc: 0.6667\n",
      "\n",
      "Best validation accuracy: 0.7333\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "# this is the main training loop that runs for epochs\n",
    "best_val_acc = 0.0\n",
    "best_model_state = None\n",
    "\n",
    "# this loops over the number of epochs\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer, device) # train for one epoch\n",
    "    val_loss, val_acc = evaluate(model, val_loader, criterion, device) # evaluate on validation set\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{NUM_EPOCHS}] \"\n",
    "          f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} \"\n",
    "          f\"|| Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "    # Save best model\n",
    "    # this saves the model if it has the best validation accuracy so far\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        best_model_state = model.state_dict()\n",
    "\n",
    "print(f\"\\nBest validation accuracy: {best_val_acc:.4f}\")\n",
    "\n",
    "# Load best model state\n",
    "# this loads the best model state after training\n",
    "if best_model_state is not None:\n",
    "    model.load_state_dict(best_model_state)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved full model to: Group_16_CNN_FullModel.ph\n"
     ]
    }
   ],
   "source": [
    "# Save the full model\n",
    "torch.save(model, MODEL_PATH)\n",
    "print(f\"Saved full model to: {MODEL_PATH}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
